<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>My Solutions to "How to scale your model" | Eitan Turok</title> <meta name="author" content="Eitan Turok"> <meta name="description" content="My Solutions to " how to scale your model> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://eitanturok.github.io/blog/2025/how-to-scale-your-model-solutions/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?6185d15ea1982787ad7f435576553d64"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "My Solutions to "How to scale your model"",
      "description": "My Solutions to "How to scale your model"",
      "published": "June 30, 2025",
      "authors": [
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Eitan Turok</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>My Solutions to "How to scale your model"</h1> <p>My Solutions to "How to scale your model"</p> </d-title><d-article> <h1 id="intro">Intro</h1> <p>I thought it would be fun to go through <a href="https://jax-ml.github.io/scaling-book/" rel="external nofollow noopener" target="_blank">How to Scale Your Model</a>, a small book on how to train and perform inference on LLMs at scale. This is not what is a forward or backward pass. This is how do you maximize MFU for an MoE on 1024 GPUs. This is the big leagues and contains a lot of knowledge that is really just kept in large industry labs or the well-connected labs of academia. There is no real textbook on this info because it is so new and “cutting edge” – that is until now. This material is “solidly untaught,” <a href="https://jacobaustin123.substack.com/p/technical-writing-with-unit-tests" rel="external nofollow noopener" target="_blank">writes</a> Jacob, one of the authors, “I don’t believe another resource exists that tells this technical story end-to-end.”</p> <p><img src="how-to-scale-your-model-screenshot.png" alt="how to scale your model screenshot"></p> <p>I’m an especially big fan of <a href="https://jax-ml.github.io/scaling-book/" rel="external nofollow noopener" target="_blank">How to Scale Your Model</a> because it has great practice problems. They are like HW problems, but they really make you think. And they are practical too.</p> <p>Over the next couple of weeks, I want to work through all 11 chapters of <a href="https://jax-ml.github.io/scaling-book/" rel="external nofollow noopener" target="_blank">How to Scale Your Model</a>. I’m going to record my solutions here.</p> <h1 id="chapter-1-solutions">Chapter 1 Solutions</h1> <h2 id="question-11">Question 1.1</h2> <p><strong>Question 1 [int8 matmul]:</strong> Say we want to do $X[B, D] \cdot_D Y[D, F] \rightarrow Z[B, F]$ in int8 precision (1 byte per parameter) instead of bfloat16.<d-footnote>Here and throughout we'll use the notation $A \cdot_D B$ to indicate that the multiplication is performing a contraction over the D dimension. This is an abuse of einsum notation.</d-footnote></p> <ol> <li>How many bytes need to be loaded from memory? How many need to be written back to memory?</li> <li>How many total OPs are performed?</li> <li>What is the arithmetic intensity?</li> <li>What is a roofline estimate for $T_\text{math}$ and $T_\text{comms}$? What are reasonable upper and lower bounds for the runtime of the whole operation?</li> </ol> <p>Assume our HBM bandwidth is <code class="language-plaintext highlighter-rouge">8.1e11</code> bytes/s and our int8 peak OPs/s is <code class="language-plaintext highlighter-rouge">3.94e14</code>.</p> <p><strong>Answer 1:</strong></p> <ol> <li>Since this is int8, each number takes up a single byte. Therefore, we load $BD$ bytes from $X$, load $DF$ bytes from $Y$, and write $BF$ bytes from $Z$. In total we must communicate $BD + DF + BF$ bytes.</li> <li>Matrix multiplication between $X$ and $Y$ costs $BF(D + (D-1)) = 2BFD$ OPs because for each of the $BF$ entries of the resultant matrix $Z$ we perform a dot product between two $D$ dimensional vectors, with $D$ multiplications and $D-1$ additions.</li> <li>\(\begin{equation} \text{Arithmetic Intensity} = \frac{\text{Computation FLOPs}}{\text{Communication Bytes}} = \frac{2BFD}{BD + DF + BF} \approx \frac{BFD}{DF} = 2B \end{equation}\). Again, we assume that the batch size $B$ is <em>much</em> less than the hidden dimensions $D, F$, i.e. $BD « DF$ and $BF « DF$ and so can ignore these two terms in the denominator.</li> <li>Let’s define \(\begin{equation} T_\text{math} = \frac{\text{Computation FLOPs}}{\text{Accelerator FLOPs/s}} = \frac{2BFD}{8.1e11} \end{equation}\) and \(\begin{equation} T_\text{comms} = \frac{\text{Communication Bytes}}{\text{Network/Memory Bandwidth Bytes/s}} = \frac{DF}{3.9e14} \end{equation}\) Notice that $T_\text{math} &gt; T_\text{comms}$ because \(\begin{equation}\end{equation}\) \(\begin{equation} T_\text{lower}=\max(T_\text{math}, T_\text{comms}) \end{equation}\)</li> </ol> <h2 id="question-12">Question 1.2</h2> <p><strong>Question 2 [int8 + bf16 matmul]:</strong> In practice we often do different weight vs. activation quantization, so we might store our weights in very low precision but keep activations (and compute) in a higher precision. Say we want to quantize our weights in int8 but keep activations (and compute) in bfloat16. At what batch size do we become compute bound? Assume <code class="language-plaintext highlighter-rouge">1.97e14</code> bfloat16 FLOPs/s.</p> <p><em>Hint: this means specifically <code class="language-plaintext highlighter-rouge">bfloat16[B, D] * int8[D, F] -&gt; bfloat16[B, F]</code> where $B$ is the “batch size”.</em></p> <p><strong>Answer 2:</strong> Each number takes up a single byte in int8 and two bytes in bfloat16. Therefore, we load $2BD$ bytes from $X$, load $DF$ bytes from $Y$, and write $2BF$ bytes from $Z$. In total we communicate $2BD + DF + 2BF$ bytes. Assume that $B«D$ and $B«F$ because the batch size $B$ is often much less than the hidden dimensions $D, F$, meaning we approximate this as communicating $DF$ bytes.</p> <p>Matrix multiplication between $X$ and $Y$ costs $BF(D + (D-1)) = 2BFD$ OPs because for each of the $BF$ entries of the resultant matrix $Z$ we perform a dot product between two $D$ dimensional vectors, with $D$ multiplications and $D-1$ additions.</p> <p>Therefore \(\begin{equation} \text{Intensity}(\text{Arithmetic}) = \frac{\text{Computation FLOPs}}{\text{Communication Bytes}} = \frac{2BFD}{BD + DF + BF} \approx \frac{2BFD}{DF} = 2B \end{equation}\)</p> \[\begin{equation} \text{Intensity(Accelerator)} = \frac{\text{Accelerator FLOPs/sec}}{\text{Accelerator Bytes/sec}} = \frac{1.97e14}{8/1e11} = 243 \end{equation}\] <p>We are compute bound when \(\begin{align*} \text{Intensity}(\text{Arithmetic}) \geq \text{Intensity}(\text{Accelerator}) \\ 2B \geq 243 \\ B \geq 121.5 \end{align*}\)</p> <h2 id="question-13">Question 1.3</h2> <p><strong>Question 3:</strong> For the problem above, make a roofline plot of peak FLOPs vs. B for several values of D and F.</p> <h2 id="question-14">Question 1.4</h2> <p><strong>Question 4:</strong> What if we wanted to perform $\text{int8[B, D]} *_D \text{int8[B, D, F]} \rightarrow \text{int8[B, F]}$ where we imagine having a different matrix for each batch element. What is the arithmetic intensity of this operation?</p> <p><strong>Answer 4:</strong> Here, the second matrix is <em>three</em> dimensional, not two dimensional. This will not effect the actual number of operations we perform, but will change the number of bytes communicated. We perform $BF(D + (D-1)) \approx 2BFD$ operations because for each of the $BF$ elements of the resultant matrix $Z$ we perform $D$ multiplications and $D-1$ additions. We load $BD$ bytes from $X$, $BDF$ bytes from $Y$, and write $BF$ bytes from $Z$, totaling $BD + BFD + BF$ bytes. Assuming that $B « D$ and $B « F$, the term $BFD$ dominates and we can effectively ignore the other two terms.</p> \[\begin{align*} \text{Intensity}(\text{Arithmetic}) = \frac{\text{Computation FLOPs}}{\text{Communication Bytes}} = \frac{BF(D + (D-1))}{BD + BFD + BF} \approx \frac{2BFD}{BFD} = 2 \end{align*}\] <p>We have a <em>constant</em> arithmetic intensity, meaning that we will always compute 2 operations per byte, no matter what our batch size is. In other words, we will always be communication bottle-necked. This is bad.</p> <h2 id="question-15">Question 1.5</h2> <p><strong>Problem 5 [Memory Rooflines for GPUs]:</strong> Using the <a href="https://www.nvidia.com/en-us/data-center/h100/" rel="external nofollow noopener" target="_blank">spec sheet provided by NVIDIA for the H100</a>, calculate the batch size at which a matrix multiplication will become compute-bound. <em>Note that the Tensor Core FLOPs numbers are twice the true value since they’re only achievable with structured sparsity.</em></p> <p><strong>Answer 5:</strong> For bfloat16 with tensor core, the H100 SXM can compute 1919 teraflops per second, i.e. 1.919e15 FLOPs/sec. Because this is with sparsity, we must divide by two to get the FLOPs/sec for non-sparse operations, 1e15 FLOPs/sec. The H100 SXM has a reported GPU memory bandwidth of 3.35 terabytes per second, i.e. 3.35e12 bytes per second. \(\begin{align*} \text{Intensity}(\text{Accelerator}) = \frac{\text{Accelerator FLOPs/sec}}{\text{Accelerator Bytes/sec}} = \frac{1e15}{3.35e12} = 298 \end{align*}\)</p> <p>Assume $\text{bfloat16[B, D]} *_D \text{bfloat16[D, F]} \rightarrow \text{bfloat16[B, F]}$ is the matrix multiplication we are performing. We load $2BD$ bytes from the first matrix, load $2DF$ bytes from the second matrix, and write $2BF$ bytes from the resultant matrix totaling $2BD + 2DF + 2BF$ bytes. Assume that $B«F$ and $B«D$ so that the $2DF$ term dominates and we can ignore the other two terms. We compute $BF(D + (D-1))$ bytes because each of the $BF$ entries in the resultant matrix requires $D$ multiplications and $D-1$ additions. For simplicty, let’s approximate this as $2BFD$. \(\begin{align*} \text{Intensity}(\text{Arithmetic}) = \frac{\text{Computed FLOPs}}{\text{Communicated Bytes}} = \frac{BF(D + (D-1))}{2BD + 2DF + 2BF} \approx \frac{2BFD}{2DF} = B \end{align*}\)</p> <p>We are compute bound when \(\begin{align*} \text{Intensity}(\text{Arithmetic}) \geq \text{Intensity}(\text{Accelerator}) \\ B \geq 298 \end{align*}\) Therefore $B_\text{crit} = 298$.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2018-12-22-distill.bib"></d-bibliography><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"eitanturok/eitanturok.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Eitan Turok. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>