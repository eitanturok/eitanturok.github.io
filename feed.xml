<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://eitanturok.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://eitanturok.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-30T10:45:02+00:00</updated><id>https://eitanturok.github.io/feed.xml</id><title type="html">Eitan Turok</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Can You Solve Dynamic Programming Problems Faster? Part 1, One-Dimensional LWS</title><link href="https://eitanturok.github.io/blog/2024/faster-dynamic-programming/" rel="alternate" type="text/html" title="Can You Solve Dynamic Programming Problems Faster? Part 1, One-Dimensional LWS"/><published>2024-01-09T15:59:00+00:00</published><updated>2024-01-09T15:59:00+00:00</updated><id>https://eitanturok.github.io/blog/2024/faster-dynamic-programming</id><content type="html" xml:base="https://eitanturok.github.io/blog/2024/faster-dynamic-programming/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>Dynamic programming (DP) is one of the most common algorithmic paradigms, used everywhere from DNA sequencing and financial modeling to matrix multiplication and shortest path algorithms. When one solves a problem using DP, a natural question arises: <em>is this the fastest algorithm to solve the problem?</em></p> <p>Recently, fine-grained complexity has been used to show that for many important problems, the standard DP algorithm <em>is</em> optimal. For example, researchers proved that for the <a href="https://leetcode.com/problems/edit-distance/description/">edit distance</a> problem, there is no algorithm (whether or not it uses DP) that is faster than the standard DP algorithm by a polynomial factor (assuming \(\text{SETH}\)).</p> <p>On the other hand, there are some notable examples where a natural DP formulation is <em>not</em> optimal. For example, researchers showed that for the <a href="https://leetcode.com/problems/minimum-score-triangulation-of-polygon/description/">polygon triangulation</a> problem, the standard DP algorithm results in an \(O(n^3)\) time solution while geometric techniques lead to an \(O(n \log n)\) time solution, a polynomial speedup.</p> <p>So, we wonder:</p> <blockquote> <p>For which problems can we achieve a polynomial speedup over the standard DP algorithm? And when is the standard DP algorithm already optimal?</p> </blockquote> <p>In our <a href="https://arxiv.org/abs/2309.04683">latest work</a>, we provide an answer to this question for certain kinds of DP problems, specifically for \(k\)-dimensional Least Weight Subsequence (\(k\text{D}\hspace{1mm}\text{LWS}\)) DP problems.</p> <p>We prove that achieving a polynomial speedup for \(k\text{D}\hspace{1mm}\text{LWS}\) depends on the cost of transitioning from one DP state to another. This cost is expressed as a matrix in two dimensions or, analogously, a tensor in higher dimensions. If this cost tensor has constant <a href="https://en.wikipedia.org/wiki/Rank_(linear_algebra)">rank</a>, then it <em>is possible</em> to achieve a polynomial speedup. But if the rank is slightly greater than constant, a polynomial speedup <em>is impossible</em> (assuming \(\text{SETH}\)).</p> <p>This is a beautiful result: if the cost tensor has a simple structure, i.e. constant rank, we can exploit it to solve the problem faster. But once the cost tensor becomes slightly more complex, i.e. super-constant rank, there is a naturally occurring barrier that makes it impossible to solve the problem any faster. This suggests there is some inherent difference between a constant and super-constant rank, a fundamental transition point where the computation goes from fast to slow.</p> <p>Informally, our main result is:</p> <blockquote> <p>For \(k\text{D}\hspace{1mm}\text{LWS}\) DP problems, we prove that a polynomial speedup over the standard DP algorithm is possible when the rank of the cost tensor is \(O(1)\) but impossible when it is \(2^{O(\log^* n)}\) or greater (assuming \(\text{SETH}\)).</p> </blockquote> <p>This post introduces \(\text{LWS}\), the one dimensional version of the \(k\text{D}\hspace{1mm}\text{LWS}\) problem. The next posts discuss the \(k\text{D}\hspace{1mm}\text{LWS}\) problem, how we proved our results, and the problems that we can now solve faster.</p> <p>Let’s dive in.</p> <h1 id="lws-dynamic-programming-in-one-dimension">LWS, Dynamic Programming in One Dimension</h1> <p>When most students learn about DP they think it is all about filling in DP tables. This is not true! The most important part of DP is finding the recurrence relation, a recursive equation that gives a solution for the problem in terms of simpler sub-problems. This is the heart of dynamic programming and a natural way to characterize different DP problems.</p> <p>\(k\text{D}\hspace{1mm}\text{LWS}\) is a class of DP problems with a certain kind of recurrence relation. To develop a working intuition of \(k\text{D}\hspace{1mm}\text{LWS}\), we will first focus on the one dimensional version of \(k\text{D}\hspace{1mm}\text{LWS}\), which we just call the \(\text{LWS}\) problem. In this section we will analyze the recurrence relations of three different DP problems and create a general recurrence relation that captures all of these problems – this will be the \(\text{LWS}\) recurrence relation!</p> <h2 id="longest-increasing-subsequence-problem">Longest Increasing Subsequence Problem</h2> <h3 id="definition">Definition</h3> <p>To begin, let’s find the recurrence relation for the <a href="https://en.wikipedia.org/wiki/Longest_increasing_subsequence">longest increasing subsequence</a> (\(\text{LIS}\)) problem:</p> <blockquote> <p>Given an integer array \(X = [x_1, \dots, x_n]\), return the length of the longest strictly increasing subsequence in this array.</p> </blockquote> <p>If our array is \(X = [10,9,2,5,3,7,101,18]\) our \(\text{LIS}\) is \([2,3,7,101]\) because this is the longest possible subsequence of integers where each element is greater than the next, i.e. where the subsequence is strictly increasing. We return \(4\) here because that is the length of our \(\text{LIS}\) \([2,3,7,101]\). The \(\text{LIS}\) problem is quite famous, having numerous research papers written about it and having been attempted on <a href="https://leetcode.com/problems/longest-increasing-subsequence/description/">LeetCode</a> over 3 million times!</p> <h3 id="recurrence-relation">Recurrence Relation</h3> <p>The recurrence relation for \(\text{LIS}\) is</p> \[dp[j] = \begin{cases} 0 &amp; \text{if $j == 0$} \\ \max_{0 \leq i &lt; j} dp[i] + \mathbb{1}[x_j &gt; x_i] &amp; \text{otherwise.} \\ \end{cases}\] <p>where \(dp[j]\) is the \(\text{LIS}\) of \(x_1, \dots, x_j\) and \(\mathbb{1}[x_j &gt; x_i]\) is an indicator variable that returns one when \(x_j &gt; x_i\) and zero otherwise. The solution to the \(\text{LIS}\) problem is given by \(dp[n]\). Now this equation is a bit dense so let’s give some intuition for the two cases.</p> <p><strong>Base Case \(j == 0\):</strong></p> <p>Since \(dp[j]\) is defined as the \(\text{LIS}\) of \(x_1, \dots, x_j\), \(dp[0]\) must be the \(\text{LIS}\) of \(x_1, \dots, x_0\). This sequence is undefined because \(x_1\) comes before \(x_0\) so we set the length of this \(\text{LIS}\) to zero.</p> <p><strong>Otherwise:</strong></p> <p>In this case \(j &gt; 0\), meaning we have one or more elements in our sequence \(x_1, \dots, x_j\). So let’s assume we have already computed \(dp[i]\), the \(\text{LIS}\) of a shorter sequence \(x_1, \dots, x_i\) where \(i &lt; j\), and use this to help us compute \(dp[j]\). If \(x_j &gt; x_i\), we can add \(x_j\) to the \(\text{LIS}\) of \(dp[i]\) and the sequence will increase by one, i.e. \(dp[j] = dp[i] + 1\). But if \(x_j \leq x_i\), we cannot add \(x_j\) to \(dp[i]\) so we just have \(dp[j] = dp[i] + 0\). Both of these cases can be succinctly represented by the term \(\mathbb{1}[x_j &gt; x_i]\).</p> <h3 id="reformat-the-recurrence-relation">Reformat the Recurrence Relation</h3> <p>Now we will reformat the \(\text{LIS}\) recurrence relation so it more closely resembles the \(\text{LWS}\) recurrence relation (more details about this later). We previously defined \(dp[j]\) as</p> \[dp[j] = \begin{cases} 0 &amp; \text{if $j == 0$} \\ \max_{0 \leq i &lt; j} dp[i] + \mathbb{1}[x_j &gt; x_i] &amp; \text{otherwise.} \\ \end{cases}\] <p>Now we will reformat \(dp[j]\) as</p> \[dp[j] = \begin{cases} 0 &amp; \text{if $j == 0$} \\ \min_{0 \leq i &lt; j} dp[i] + w[i, j] &amp; \text{otherwise.} \\ \end{cases}\] <p>where \(w[i, j]\) is an \(n \times n\) matrix defined as</p> \[w[i, j] = \begin{cases} -1 &amp; \text{if $x_j &gt; x_i$} \\ 0 &amp; \text{otherwise.} \\ \end{cases}\] <p>Notice that \(w[i, j]\) equals negative one when \(x_j\) can be added to a subsequence which ends in \(x_i\), thus increasing the length of the \(\text{LWS}\) by \(1\). Since \(\text{LIS}\) is a maximization problem and \(\text{LWS}\) is a minimization problem, the weights are \(-1\), not \(1\), and the solution is given by \(−dp[n]\) instead of \(dp[n]\). We’ll keep the \(\text{LIS}\) recurrence relation in this modified format and come back to it later.</p> <h2 id="coin-change-problem">Coin Change Problem</h2> <h3 id="definition-1">Definition</h3> <p>Next, let’s find the recurrence relation for the <a href="https://en.wikipedia.org/wiki/Change-making_problem">coin change</a> (\(\text{CC}\)) problem:</p> <blockquote> <p>Given an integer array of \(m\) coins \(C = [c_1, \dots, c_m]\) where \(c_i\) is a coin worth \(c_i\) cents, return the fewest number of coins needed to make \(n\) cents. (Assume you have an infinite number of each kind of coin.)</p> </blockquote> <p>If we have coins \([1, 2, 5]\) and want to make a total of \(n=11\) cents, we would return \(3\) because using two \(5\)-cent coins and one \(1\)-cent coin makes \(11\) cents with the fewest number of coins. The \(\text{CC}\) problem is also quite famous, having been studied in tons of research papers and having been attempted on <a href="https://leetcode.com/problems/coin-change/description/">LeetCode</a> over 4 million times!</p> <h3 id="recurrence-relation-1">Recurrence Relation</h3> <p>The recurrence relation for this problem is</p> \[dp[j] = \begin{cases} 0 &amp; \text{if $j == 0$} \\ \infty &amp; \text{if $j &lt; 0$} \\ \min_{1 \leq i \leq m} dp[j-c_i] + 1 &amp; \text{otherwise.} \\ \end{cases}\] <p>where \(dp[j]\) is the minimum number of coins needed to make \(j\) cents. The solution to the \(\text{CC}\) problem is given by \(dp[n]\). If \(dp[n] == \infty\), then it is not possible to make \(n\) cents using the coins in array \(C\). Again, this equation may look a bit dense so let’s break down the three cases:</p> <p><strong>Base case \(j == 0\):</strong></p> <p>The minimum number of coins to make \(j = 0\) cents is \(0\) because you need \(0\) coins to make \(0\) cents. Pretty simple.</p> <p><strong>Base case \(j &lt; 0\):</strong></p> <p>The minimum number of coins needed to make a negative number of cents, i.e. \(j &lt; 0\), is undefined. And because this is a minimization problem, we set this to \(\infty\) to ensure this choice is not chosen.</p> <p><strong>Otherwise:</strong></p> <p>In this case we have need to make \(j\) cents with our coins $C$. So imagine we select a coin worth \(c_i\) cents. Then we only need to make another \(j-c_i\) cents to make \(j\) total cents. To find the minimum number of coins needed to make the remaining \(j-c_i\) cents we simply need to know \(dp[j-c_i]\). Our expression has a \(+1\) because by using the coin \(c_i\), we’ve increased the number of coins we’ve used by one, giving us \(dp[j-c_i] + 1\).</p> <h3 id="reformat-the-recurrence-relation-1">Reformat the Recurrence Relation</h3> <p>Now we will reformat the \(\text{CC}\) recurrence relation so it more closely resembles the \(\text{LWS}\) recurrence relation (more details about this later). Previously, we defined \(dp[j]\) as</p> \[dp[j] = \begin{cases} 0 &amp; \text{if $j == 0$} \\ \infty &amp; \text{if $j &lt; 0$} \\ \min_{1 \leq i \leq m} dp[j-c_i] + 1 &amp; \text{otherwise.} \\ \end{cases}\] <p>Now we will reformat \(dp[j]\) as</p> \[dp[j] = \begin{cases} 0 &amp; \text{if $j == 0$} \\ \min_{0 \leq i &lt; j} dp[i] + w[i, j] &amp; \text{otherwise.} \\ \end{cases}\] <p>where \(w[i, j]\) is an \(n \times n\) matrix defined as</p> \[w[i, j] = \begin{cases} 1 &amp; \text{if $j-i \in C$} \\ \infty &amp; \text{otherwise.} \\ \end{cases}\] <p>and where solution to \(\text{CC}\) is given by \(dp[n]\).</p> <p>Instead of having \(dp[j-c_i]\) and \(\min_{1 \leq i \leq m}\) our new recurrence relation has \(dp[i]\) and \(\min_{0 \leq i &lt; j}\).</p> <p>The condition \(j-i \in C\) means we add a one if there a coin worth \(j-i\) cents in our array of coins \(C\) because we can only go from having \(j\) cents to having \(i\) cents if a coin worth \(j-i\) cents exist. If no coin worth \(j - i\) cents exists, we cannot go from \(j\) cents to \(i\) cents and thus assign an \(\infty\) value to avoid this kind of case.</p> <p>We’ll keep the \(\text{CC}\) recurrence relation in this modified format and come back to it later.</p> <h2 id="airplane-refueling-problem">Airplane Refueling Problem</h2> <h3 id="definition-2">Definition</h3> <p>Lastly, let’s find the recurrence relation for the <a href="https://leetcode.com/problems/minimum-number-of-refueling-stops/description/">airplane refueling</a> (\(\text{AR}\)) problem:</p> <blockquote> <p>Suppose an airplane is flying from source \(x_0\) to destination \(x_n\). Given a list of optional refueling stations at positions \(X = [x_1, \dots, x_n]\), return the minimum cost way to fly from \(x_0\) to \(x_n\). It costs \(([x_j - x_i] - l)^2\) to fly from airport \(x_i\) to airport \(x_j\) where \(l\) is the optimal distance for the plane to travel for fuel efficiency reasons.</p> </blockquote> <p>Let’s make two assumptions:</p> <ul> <li>Let \(0 = x_0 &lt; x_1 &lt; \dots &lt; x_{n-1} &lt; x_n\) so that the source is at position \(0\) and the airports are sorted by their distance from the source.</li> <li>The airports are located along a straight line such that distance between airport \(x_j\) and airport \(x_i\) is \(x_i - x_j\).</li> </ul> <p>If the optional refueling airports are located at positions \(X=[1, 5, 7, 10]\) and we prefer to travel \(l=3\) miles at a time, the minimum cost of traveling from the source \(x_0 = 0\) to \(x_4 = 10\) is ?.</p> <h3 id="recurrence-relation-2">Recurrence Relation</h3> <p>The recurrence relation for \(\text{AR}\) is</p> \[dp[j] = \begin{cases} 0 &amp; \text{if $j == 0$} \\ \max_{1 \leq i &lt; j} dp[i] + ([x_j - x_i] - l)^2 &amp; \text{otherwise.} \\ \end{cases}\] <p>where \(dp[j]\) is the minimum cost way to fly from source \(x_0\) to airport \(x_j\). The solution to the \(\text{AR}\) problem is given by \(dp[n]\). Let’s break down the two cases:</p> <p><strong>Base Case \(j == 0\):</strong></p> <p>When \(j == 0\), we are located at airport \(x_0\) and it costs \(0\) to go from source \(x_0\) to destination \(x_j = x_0\).</p> <p><strong>Otherwise:</strong></p> <p>To compute \(dp[j]\), the cheapest way of flying from \(x_0\) to \(x_j\), we take the cheapest combination of</p> <ul> <li>the cost of flying from \(x_i\) to \(x_j\), \(([x_j - x_i] - l)^2\)</li> <li>plus the cheapest possible cost of flying from \(x_0\) to \(x_i\), \(dp[i]\).</li> </ul> <h3 id="reformat-the-recurrence-relation-2">Reformat the Recurrence Relation</h3> <p>Now we will reformat the \(\text{CC}\) recurrence relation so it more closely resembles the \(\text{LWS}\) recurrence relation (more details about this later). Previously, we defined \(dp[j]\) as</p> \[dp[j] = \begin{cases} 0 &amp; \text{if $j == 0$} \\ \max_{1 \leq i &lt; j} dp[i] + ([x_j - x_i] - l)^2 &amp; \text{otherwise.} \\ \end{cases}\] <p>Now we will reformat \(dp[j]\) as</p> \[dp[j] = \begin{cases} 0 &amp; \text{if $j == 0$} \\ \min_{0 \leq i &lt; j} dp[i] + w[i, j] &amp; \text{otherwise.} \\ \end{cases}\] <p>where \(w[i, j]\) is an \(n \times n\) matrix defined as</p> \[w[i, j] = ([x_j - x_i] - l)^2.\] <p>The cost \(w[i, j]\) is the simply the cost of going from airport \(x_i\) to airport \(x_j\).</p> <h2 id="putting-it-all-together">Putting It All Together</h2> <p>Let’s take another look at our three reformatted recurrence relations:</p> <p><strong>Longest Increasing Subsequence \(\text{LIS}\):</strong></p> \[dp[j] = \begin{cases} 0 &amp; \text{if $j == 0$} \\ \min_{0 \leq i &lt; j} dp[i] + w[i, j] &amp; \text{otherwise.} \\ \end{cases} \ \ \ \ \ \ \ \ \text{ where } w[i, j] = \begin{cases} -1 &amp; \text{if $x_j &gt; x_i$} \\ 0 &amp; \text{otherwise.} \\ \end{cases}\] <p><strong>Coin Change \(\text{CC}\):</strong></p> \[dp[j] = \begin{cases} 0 &amp; \text{if $j == 0$} \\ \min_{0 \leq i &lt; j} dp[i] + w[i, j] &amp; \text{otherwise.} \\ \end{cases} \ \ \ \ \ \ \ \ \text{ where } w[i, j] = \begin{cases} 1 &amp; \text{if $j-i \in C$} \\ \infty &amp; \text{otherwise.} \\ \end{cases}\] <p><strong>Airplane Refueling \(\text{AR}\):</strong></p> \[dp[j] = \begin{cases} 0 &amp; \text{if $j == 0$} \\ \min_{0 \leq i &lt; j} dp[i] + w[i, j] &amp; \text{otherwise.} \\ \end{cases} \ \ \ \ \ \ \ \ \text{ where } w[i, j] = ([x_j - x_i] - l)^2\] <p>These three recurrence relations are all the exact same, except for their cost matrices \(w\). Interesting – what does this mean?</p> <p>In 1985 two researchers Daniel Hirschberg and Lawrence Larmore noticed the very same thing: numerous DP problems have identical recurrence relations and differ only in the cost matrix \(w\). Could it be that many famous DP problems are the exact same except for \(w[i, j]\), the cost of transitioning from one item \(x_i\) to another \(x_j\)?</p> <p>Hirschberg and Larmore noticed that this recurrence relation has a fundamental structure: given a sequence of items, this recurrence relation tries to find the <em>subsequence</em> of items which have the <em>minimum weight</em> when traversing the cost matrix \(w\) across our recursive calls. For instance:</p> <ul> <li>\(\text{LIS}\) wants the longest subsequence of integers that is strictly increasing.</li> <li>\(\text{CC}\) wants the smallest subsequence of coins that equal \(n\) cents.</li> <li>\(\text{AR}\) wants the cheapest subsequence of airports that allow us to fly from our source to our destination.</li> </ul> <p>All of these DP problems want to find the minimum weight subsequence of items. Hirschberg and Larmore thus named this DP problem the <em>least weight subsequence</em> problem or \(\text{LWS}\).</p> <p>Formally, \(\text{LWS}\) is defined as follows:</p> <blockquote> <p>Given \(n\) items \(X = [x_1, \dots, x_n]\) and a \(n \times n\) cost matrix \(w\) where \(w[i, j]\) depends on \(x_i, x_j\), compute the value \(dp[n]\) given the recurrence relation</p> \[dp[j] = \begin{cases} 0 &amp; \text{if $j == 0$} \\ \min_{0 \leq i &lt; j} dp[i] + w[i, j] &amp; \text{otherwise.} \\ \end{cases}\] </blockquote> <p>In this recurrence relation, \(dp[j]\) is the cheapest way of “getting” to item \(x_j\). To compute \(dp[j]\) we find the minimum over all previously computed values (\(dp[i]\)) plus the cost of transitioning from item \(x_i\) to item \(x_j\) (\(w[i, j]\)).</p> <p>Here is where the <em>subsequence</em> part of \(\text{LWS}\) comes into play: to compute \(dp[n]\), we first find the item \(x_{i_1}\) with the minimum \(dp[i_1] + w[i_1, n]\) value and include it in the our final answer’s subsequence. Then to compute \(dp[i_1]\) we find the next item \(x_{i_2}\) that results in the minimum \(dp[i_2] + w[i_2, i_1]\) value and include it in our final answer’s subsequence. We repeat the pattern and end up with a subsequence of items \(x_{i_1}, x_{i_2}, \dots\) that result in the minimum cost value for \(dp[n]\). This subsequence \(x_{i_1}, x_{i_2}, \dots\) is our least weight <em>subsequence</em>.</p> <p>Taking a step back, Hirschberg and Larmore noticed that by appropriately setting the cost matrix \(w\), they can formulate many famous DP problems as instances of \(\text{LWS}\). Indeed, by defining the cost matrix \(w\) carefully we can turn the general \(\text{LWS}\) recurrence relation into the recurrence relation for \(\text{LIS}\), \(\text{CC}\), or \(\text{AR}\)! There are dozens of other important DP problems that can be expressed through the \(\text{LWS}\) recurrence relation. Formally, \(\text{LWS}\) defines a general class of recurrence relations. Ultimately, Hirschberg and Larmore found that \(\text{LWS}\) was a fundamental problem in DP where numerous DP problems have the same underlying structure and are simply \(\text{LWS}\) problems in disguise. What a beautiful result…</p> <h1 id="solving-lws-faster">Solving LWS Faster</h1> <p>The straightforward DP algorithm solves \(\text{LWS}\) problem in \(O(n^2)\) time. This is because we have \(n\) entries in our \(dp\) table (remember, we are solving for \(dp[n]\)) and each entry takes \(O(n)\) time to compute (each entry takes the minimum over \(0 \leq i &lt; j\) and in the worst case \(j == n\) so we loop over \(O(n)\) values). So to solve \(\text{LWS}\), we must do \(n \cdot O(n) = O(n^2)\) computations. Moreover, since the cost matrix \(w\) has \(n^2\) entries, it requires quadratic time to read in the input, so a faster algorithm isn’t possible in general.</p> <p>However, in 2017 three researchers from UC San Diego – Marvin Künnemann, Ramamohan Paturi, and Stefan Schneider – challenged this assumption. They noticed that if one can input \(w\) in a more compact form, perhaps a faster algorithm would be possible.</p> <details> <summary>Refresher: What is a low-rank matrix? </summary> A matrix is low rank if it has redundant information and can be represented more compactly by a combination of a few core, underlying patterns. Formally, a matrix $$C \in \mathbb{R}^{n \times n}$$ is low rank if $$C = A \times B^T$$ where $$A,B \in \mathbb{R}^{n \times r}$$ and the rank $$r &lt;&lt; n$$. Matrix $$C$$ has $$n^2$$ entries but it can be represented by the multiplication of matrices $$A,B$$ which have a total of $$2 n \times r$$ elements; together $$A,B$$ have way fewer elements than $$C$$ because $$2 n \times r &lt;&lt; n^2$$. Intuitively, we are exploiting the structure in matrix $$C$$ and "compressing" it into a series of interactions between two smaller matrices $$A,B$$. Low-rank matrices often appear in real-world applications where there is redundant information or a simpler structure is at play. </details> <p>Künnemann et al. focused on the case where the cost matrix \(w\) is a low-rank matrix. If the cost matrix \(w\) has rank \(r &lt; n^{o(1)}\), then instead of directly inputting \(w\) into \(\text{LWS}\), we can input the \(n \times r\) matrices \(A, B\) into \(\text{LWS}\) where \(w = A \times B^T\). This reduces the input size of the problem from \(n^2\) to \(n^{1 + o(1)}\). With a much smaller input size, it is now possible to maybe come up with a faster algorithm for \(\text{LWS}\). (For the informal reader, \(o(1)\) in this context means some constant less than \(1\).)</p> <p>Can we solve any of our \(\text{LWS}\) problems faster by exploiting their low-rank cost matrix \(w\)?</p> <h3 id="textlis">\(\text{LIS}\)</h3> <p>Consider the array \(X = [7, 4, 7]\). Our \(\text{LIS}\) here is \([4, 7]\) and so we’d return \(2\), the length of this \(\text{LIS}\). Recall that the cost matrix \(w\) for \(\text{LIS}\) is defined as:</p> <p>\(w[i, j] = \begin{cases} -1 &amp; \text{if\)x_j &gt; x_i\(} \\ 0 &amp; \text{otherwise.} \\ \end{cases}\)</p> <p>In English, this means we place a \(-1\) at position \([i, j]\) of the cost matrix if \(x_j &gt; x_i\); else we place a \(0\) there. Recall that \(w\) is a zero-indexed matrix where \(i\) determines one’s position on the \(y\text{-axis}\) and \(j\) determines one’s position on the \(x\text{-axis}\). In this example,</p> \[w = \begin{bmatrix} 0 &amp; 0 &amp; 0 \\ -1 &amp; 0 &amp; -1 \\ 0 &amp; 0 &amp; 0 \\ \end{bmatrix}\] <p>The first row (\(i=0\)) has \(x_i = x_0 = 7\) and the inequality \(x_j &gt; x_i\) from the definition of \(w\) becomes \(x_j &gt; 7\). Since \(7\) is the greatest number in \(X = [7, 4, 7]\), \(x_j &gt; 7\) is never satisfied as there is no number greater than \(7\). So we place a \(0\) in every column of the first row. The same logic applies to the third row (\(i=2\)) where we have another \(7\) (\(x_i = x_2 = 7\)).</p> <p>The second row (\(i=1\)) has \(x_i = x_1 = 4\) and the inequality \(x_j &gt; x_i\) becomes \(x_j &gt; 4\). In \(X = [7, 4, 7]\), the two \(7\)’s are greater than \(4\) and so the inequality is satisfied and we place a \(-1\) in columns \(0, 2\). When we are in column \(1\), \(i,j\) both equal \(1\) and \(x_i, x_j\) have the same value. So the inequality \(x_j &gt; x_i\) is <em>not</em> satisfied and we place a \(0\) in column \(1\).</p> <p>In this particular example, the cost matrix \(w\) <em>is</em> a low-rank matrix:</p> \[wdoc = \begin{bmatrix} 0 &amp; 0 &amp; 0 \\ -1 &amp; 0 &amp; -1 \\ 0 &amp; 0 &amp; 0 \\ \end{bmatrix}^{n \times n} = \begin{bmatrix} 0 \\ 1 \\ 0 \\ \end{bmatrix}^{n \times r} \times \begin{bmatrix} -1 &amp; 0 &amp; -1 \\ \end{bmatrix}^{r \times n}\] <p>Here, \(w\) has rank \(r = 1\), meaning \(w\) can be represented more compactly: the input size is reduced from \(9\) elements to \(6\) elements! Moreover, because the cost matrix \(w\) is low-rank, it is possible to solve this \(\text{LIS}\) problem faster than the straightforward DP algorithm.</p> <p>However, \(w\) being low-rank depends on the particular values of \(X\). In general, this problem cannot be considered low-rank.</p> <blockquote> <p>Explain more why this is not in general a low-rank matrix. Specifically, we are interested in low-rank matrices that have constant rank, a rank that does not depend on \(n\).</p> </blockquote> <h3 id="coin-change">Coin Change</h3> <p>Consider the array,</p> <p>Assuming \(w\) is low-rank, Künnemann et al. show that you can solve \(\text{LWS}\) in \(O(n^{2 - 1/r})\) time, a polynomial speedup over the standard \(O(n^2)\) runtime of \(\text{LWS}\)!</p> <p>this problem reduces to the minimum inner product \(\text{Min-IP}\) problem:</p> <p>If you sort the LWS problem, it becomes low rank!</p> <h1 id="what-is-ktextdhspace1mmtextlws">What is \(k\text{D}\hspace{1mm}\text{LWS}\)?</h1> <h2 id="grid-airplane-refueling-problem">Grid Airplane Refueling Problem</h2> <h2 id="apsp">APSP</h2> <p>\(dp[i, j] = \begin{cases} 0 &amp; \text{if\)j - i == 0\(} \\ \min \begin{cases} d(i, j) \\ \min_{i &lt; k &lt; j} dp[i, k] + dp[k, j] \end{cases} &amp; \text{otherwise.} \end{cases}\)</p> <p>\(dp[i, j] = \begin{cases} 0 &amp; \text{if\)j - i == 0\(} \\ \min &amp; \text{otherwise.} \\ \end{cases}\)</p> <p>where</p> <p>\(w[i, j, k] = \begin{cases} - \infty &amp; \text{if\)j - i == 0\(} \\ d(i, j) &amp; \text{otherwise.} \\ \end{cases}\)</p> <p>Many of the problems I mention have links – not to their Wikipedia pages but to their Leetcode pages. This is because many of the problems I discuss are 1) well-known and 2) useful, practical problems. Despite the math and the proofs, this paper does not live in theory alone. It is motivated by real-world problems.</p> <p>Make note about doing research with awesome collaborators.</p>]]></content><author><name></name></author><category term="explain-paper"/><category term="dynamic-programming"/><category term="cs-theory"/><category term="algorithms"/><category term="complexity"/><category term="comments"/><summary type="html"><![CDATA[We prove you can solve dynamic programming problems polynomially faster if you have a simple cost function for kD LWS problems.]]></summary></entry><entry><title type="html">LLM Inference + FLOPs Interview Questions</title><link href="https://eitanturok.github.io/blog/2024/llm-inference-notes/" rel="alternate" type="text/html" title="LLM Inference + FLOPs Interview Questions"/><published>2024-01-09T15:59:00+00:00</published><updated>2024-01-09T15:59:00+00:00</updated><id>https://eitanturok.github.io/blog/2024/llm-inference-notes</id><content type="html" xml:base="https://eitanturok.github.io/blog/2024/llm-inference-notes/"><![CDATA[<h1 id="kv-cache--attention-complete-review-sheet">KV Cache &amp; Attention: Complete Review Sheet</h1> <p>With Claude’s help, I wrote some quick notes on LLM Inference: FLOPs, attention, and the kv-cache. Then we have 30 great interview questions about these topics. I tried making the questions tricky, beyond surface level questions. Give them a shot and let me know what you think!</p> <h1 id="notes">Notes</h1> <h2 id="flops">FLOPS</h2> <p><strong>FLOPS (Floating Point Operations Per Second)</strong> counts arithmetic operations (add, multiply, etc.).</p> <p><strong>Matrix-Vector Multiplication:</strong> A(m×k) @ v(k×1) = <code class="language-plaintext highlighter-rouge">2×m×k</code> FLOPS (k multiply-adds for each of m output elements). The factor of 2 comes from each output element requiring k multiplications + k additions.</p> <p><strong>Matrix-Matrix Multiplication:</strong> A(m×k) @ B(k×n) = <code class="language-plaintext highlighter-rouge">2×m×k×n</code> FLOPS (k multiply-adds for each of m×n output elements). The factor of 2 comes from each output element requiring k multiplications + k additions.</p> <p><strong>Compute Bound (FLOPs Bound):</strong> The bottleneck is arithmetic operations. GPU compute units are fully utilized, but memory can supply data faster than it’s consumed. Adding more compute power would speed up the process.</p> <p><strong>Memory Bound (Bandwidth Bound):</strong> The bottleneck is data movement. GPU compute units are idle waiting for data from memory. Memory bandwidth cannot supply data fast enough for the available compute. Adding more memory bandwidth would speed up the process.</p> <h2 id="attention">Attention</h2> <p><strong>Parameter Definitions:</strong></p> <ul> <li><strong>B:</strong> batch size (number of sequences processed in parallel)</li> <li><strong>L:</strong> sequence length (number of tokens in each sequence)</li> <li><strong>d_model:</strong> model dimension (hidden size, e.g., 4096 for large models)</li> </ul> <p><strong>Matrix Dimensions &amp; Descriptions:</strong></p> <ul> <li><strong>X (input tokens):</strong> (B, L, d_model) - batch of token sequences</li> <li><strong>W_q (query weights):</strong> (d_model, d_model) - learned projection for queries</li> <li><strong>W_k (key weights):</strong> (d_model, d_model) - learned projection for keys</li> <li><strong>W_v (value weights):</strong> (d_model, d_model) - learned projection for values</li> <li><strong>Q (queries):</strong> (B, L, d_model) - what each token is looking for</li> <li><strong>K (keys):</strong> (B, L, d_model) - what each token represents</li> <li><strong>V (values):</strong> (B, L, d_model) - information content of each token</li> </ul> <p><strong>Full Attention Equation:</strong> Attention(Q,K,V) = softmax(QK^T / √d_model) @ V</p> <p><strong>FLOPS Breakdown:</strong></p> <ul> <li>Q = X @ W_q: <code class="language-plaintext highlighter-rouge">2 × B × L × d_model²</code> FLOPS</li> <li>K = X @ W_k: <code class="language-plaintext highlighter-rouge">2 × B × L × d_model²</code> FLOPS</li> <li>V = X @ W_v: <code class="language-plaintext highlighter-rouge">2 × B × L × d_model²</code> FLOPS</li> <li>QK^T: <code class="language-plaintext highlighter-rouge">2 × B × L² × d_model</code> FLOPS</li> <li>softmax(QK^T / √d_model): <code class="language-plaintext highlighter-rouge">C₁ × B × L²</code> FLOPS (constant operations per element)</li> <li>softmax(scores) @ V: <code class="language-plaintext highlighter-rouge">2 × B × L² × d_model</code> FLOPS</li> </ul> <p><strong>Total FLOPS:</strong> <code class="language-plaintext highlighter-rouge">6 × B × L × d_model² + 4 × B × L² × d_model + C₁ × B × L²</code></p> <p><strong>KV Cache Impact:</strong> Only the first term <code class="language-plaintext highlighter-rouge">6 × B × L × d_model²</code> changes with KV caching. The terms <code class="language-plaintext highlighter-rouge">4 × B × L² × d_model + C₁ × B × L²</code> do not change with KV cache. For simplicity, we only focus on this firt term in the rest of this post. But in practice you must add back the two unchanging terms.</p> <h2 id="kv-cache">KV Cache</h2> <p><strong>KV Cache</strong> stores the computed key and value vectors from previous tokens during autoregressive generation, avoiding redundant recomputation. Instead of recalculating K and V for all tokens at each step, we cache them and only compute new entries.</p> <p><strong>Example</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Prompt: "The weather today"
Target: Generate " is sunny"

Prefill Phase:
- Input: ["The", "weather", "today"] (3 tokens)
- Compute: K₁,V₁, K₂,V₂, K₃,V₃ in parallel
- Cache: Store all 3 K,V pairs
- Time: Fast parallel processing

Generation Phase:
Token 4 " is":
- Read: K₁,V₁, K₂,V₂, K₃,V₃ from cache
- Compute: Q₄, K₄, V₄
- Attention: Q₄ @ [K₁,K₂,K₃,K₄]^T @ [V₁,V₂,V₃,V₄]
- Cache: Append K₄,V₄

Token 5 " sunny":
- Read: K₁,V₁, K₂,V₂, K₃,V₃, K₄,V₄ from cache
- Compute: Q₅, K₅, V₅
- Attention: Q₅ @ [K₁,K₂,K₃,K₄,K₅]^T @ [V₁,V₂,V₃,V₄,V₅]
- Cache: Append K₅,V₅
</code></pre></div></div> <p><strong>Implementation</strong> To store the previous keys and value, the KV cache is defined as a tensor</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cache = Tensor.empty(2, B, L, n_layers, n_heads, d_head)
</code></pre></div></div> <p>where:</p> <ul> <li>2: separate storage for keys and values</li> <li>B, L: the number of tokens (L) across all sequences (B)</li> <li>n_layers, n_heads, d_head: the number of keys and value we have across all layers of a model where the keys are <code class="language-plaintext highlighter-rouge">cache[0]</code> and the values are <code class="language-plaintext highlighter-rouge">cache[1]</code>. The kv cache works by reading from and writing to this tensor.</li> </ul> <p>To find the key from the 6th token in the 3rd sequence at the 9th head from 14th layer of the model, we would do</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cache[0, 3, 6, 14, 9]
</code></pre></div></div> <p>Naively, this kind of naive KV-cache only works for greedy sampling and it does not work for more complicated sampling schemes like min-p or beam decoding.</p> <p><strong>Memory Requirement:</strong> This requires storing <code class="language-plaintext highlighter-rouge">2 × B × L × n_layers × n_heads × d_head × n_bytes</code> total bytes in memory because the cache is a tensor of shape <code class="language-plaintext highlighter-rouge">2, B, L, n_layers, n_heads, d_head</code> and each element in that tensor takes up <code class="language-plaintext highlighter-rouge">n_bytes</code>.</p> <p><strong>Prefill Phase (Time to First Token - TTFT):</strong></p> <ul> <li>Process entire input prompt in parallel</li> <li>Compute K,V for ALL input tokens simultaneously: O(L_input)</li> <li>Initialize KV cache with input token representations</li> <li>High memory bandwidth and compute utilization</li> <li>TTFT dominated by this parallel processing of input</li> </ul> <p><strong>Generation Phase (Time Between Tokens):</strong></p> <ul> <li>Process one new token at a time autoregressively</li> <li>Compute K,V only for new token: O(1) per token</li> <li>Read entire cached history for attention: O(L_total)</li> <li>Memory bandwidth becomes bottleneck as sequence grows</li> </ul> <p><strong>When FLOPS Bound (KV Cache Helps):</strong></p> <ul> <li>Without cache: Spend time computing Q,K,V for all tokens (6 × d_model² FLOPS)</li> <li>With cache: Only compute Q for new token (2 × d_model² FLOPS)</li> <li><strong>Result</strong>: 3× speedup because we eliminated computational work</li> </ul> <p><strong>When Memory Bound (KV Cache May Hurt):</strong></p> <ul> <li>Without cache: Read input tokens, compute everything fresh</li> <li>With cache: Read massive amounts of cached K,V data plus compute Q</li> <li><strong>Result</strong>: More memory traffic, potentially slower despite less computation</li> </ul> <p><strong>Example</strong>: Long sequences (1000+ tokens) often become memory bound because reading cached data dominates the time, making the FLOPS savings irrelevant.</p> <h3 id="kv-cache-pros-and-cons">KV Cache Pros and Cons</h3> <table> <thead> <tr> <th><strong>Advantages</strong></th> <th><strong>Disadvantages</strong></th> </tr> </thead> <tbody> <tr> <td><strong>3× FLOPS reduction</strong> for Q,K,V computation</td> <td><strong>Linear memory growth</strong> with sequence length</td> </tr> <tr> <td><strong>Enables real-time generation</strong> for interactive applications</td> <td><strong>Reduces maximum batch size</strong> due to memory constraints</td> </tr> <tr> <td><strong>Critical for long context</strong> models (would be unusably slow otherwise)</td> <td><strong>Memory bandwidth bottleneck</strong> for very long sequences</td> </tr> <tr> <td><strong>Powers production chat systems</strong> at scale</td> <td><strong>Beam search memory explosion</strong> when caches are copied</td> </tr> <tr> <td><strong>Amortizes compute cost</strong> over conversation length</td> <td><strong>No benefit for training</strong> (only inference optimization)</td> </tr> <tr> <td><strong>Enables streaming responses</strong> for better UX</td> <td><strong>Overhead for short sequences</strong> (&lt; 50 tokens)</td> </tr> <tr> <td><strong>Facilitates longer conversations</strong> without timeout</td> <td><strong>Cache sharing impossible</strong> between different users</td> </tr> </tbody> </table> <h3 id="flops-analysis">FLOPS Analysis</h3> <table> <thead> <tr> <th>Generation</th> <th>Without KV Cache</th> <th>With KV Cache</th> <th>Explanation</th> </tr> </thead> <tbody> <tr> <td><strong>Single token (Lth token, batch size B)</strong></td> <td><code class="language-plaintext highlighter-rouge">3 × 2 × d_model² × B + C</code></td> <td><code class="language-plaintext highlighter-rouge">2 × d_model² × B + C</code></td> <td>Without: compute Q,K,V from scratch (3 matrices × 2 FLOPS factor). With: only compute Q, reuse cached K,V. C = attention matrix operations (same for both)</td> </tr> <tr> <td><strong>Cumulative tokens (tokens 1, …, L, batch size B)</strong></td> <td><code class="language-plaintext highlighter-rouge">3 × 2 × d_model² × B × L + C × L</code></td> <td><code class="language-plaintext highlighter-rouge">2 × d_model² × B × L + C × L</code></td> <td>Without: recompute Q,K,V for every token. With: compute Q for each new token only. C scales with sequence length</td> </tr> </tbody> </table> <h3 id="kv-cache-memory--io-only-relevant-with-caching">KV Cache Memory &amp; I/O (Only Relevant With Caching)</h3> <table> <thead> <tr> <th>Generation</th> <th>Memory Stored</th> <th>Bandwidth: Read</th> <th>Bandwidth: Write</th> <th>Explanation</th> </tr> </thead> <tbody> <tr> <td><strong>Single token (Lth token, batch size B)</strong></td> <td><code class="language-plaintext highlighter-rouge">2 × n_bytes × n_layers × n_heads × d_head × B × L</code></td> <td><code class="language-plaintext highlighter-rouge">2 × n_bytes × n_layers × n_heads × d_head × B × (L-1)</code></td> <td><code class="language-plaintext highlighter-rouge">2 × n_bytes × n_layers × n_heads × d_head × B</code></td> <td>Memory: total cache size grows with L. Read: access all previous tokens for attention. Write: store current token’s K,V</td> </tr> <tr> <td><strong>Cumulative tokens (tokens 1, …, L, batch size B)</strong></td> <td><code class="language-plaintext highlighter-rouge">2 × n_bytes × n_layers × n_heads × d_head × B × L</code></td> <td><code class="language-plaintext highlighter-rouge">2 × n_bytes × n_layers × n_heads × d_head × B × L × (L-1)/2</code></td> <td><code class="language-plaintext highlighter-rouge">2 × n_bytes × n_layers × n_heads × d_head × B × L</code></td> <td>Memory: same final size. Read: cumulative bandwidth = 0+1+2+…+(L-1) = L(L-1)/2 per batch. Write: one entry per token generated</td> </tr> </tbody> </table> <h3 id="beam-search-with-kv-cache">Beam Search with KV Cache</h3> <p><strong>Definition:</strong> Beam search maintains the k most promising sequences at each step, exploring multiple paths simultaneously to find higher-quality outputs than greedy decoding.</p> <p><strong>Algorithm:</strong></p> <ol> <li>Start with k beams (initially just the prompt)</li> <li>For each beam, generate top-r candidate next tokens</li> <li>Score all k×r candidates</li> <li>Keep top-k sequences as new beams</li> <li>Repeat until completion</li> </ol> <p><strong>Concrete Example with KV Cache:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model: 2 layers, 4 heads, d_head=64, n_bytes=2
Beam width k=2, candidates per beam r=3
Starting sequence: "The weather" (2 tokens)

Initial State:
Beam 1: "The weather"
KV Cache 1: K₁,V₁ (for "The"), K₂,V₂ (for "weather")
Cache size: 2×2×2×4×64 = 1,024 bytes

Beam 2: "The weather"
KV Cache 2: K₁,V₁ (for "The"), K₂,V₂ (for "weather")
Cache size: 1,024 bytes (same starting point)

Step 1: Generate third token (each beam explores 3 candidates)
Beam 1 candidates: "is" (0.6), "was" (0.3), "feels" (0.2)
Beam 2 candidates: "today" (0.5), "looks" (0.4), "seems" (0.1)

Step 2: Select top-2 sequences across all 6 candidates
Selected: "The weather is" (0.6), "The weather today" (0.5)

MEMORY EXPLOSION POINT:
New Beam 1: "The weather is"
- Needs: COPY of original Beam 1 cache + new K₃,V₃ for "is"
- Cache: K₁,V₁, K₂,V₂, K₃ⁱˢ,V₃ⁱˢ = 1,536 bytes

New Beam 2: "The weather today"
- Needs: COPY of original Beam 2 cache + new K₃,V₃ for "today"
- Cache: K₁,V₁, K₂,V₂, K₃ᵗᵒᵈᵃʸ,V₃ᵗᵒᵈᵃʸ = 1,536 bytes

Total Memory: 3,072 bytes (3× explosion from beam splitting!)
Note: The beams started identical but diverged due to different candidate selection
</code></pre></div></div> <h3 id="beam-search-flops--memory-analysis">Beam Search FLOPS &amp; Memory Analysis</h3> <table> <thead> <tr> <th>Generation</th> <th>FLOPS (vs Greedy)</th> <th>Memory Stored</th> <th>Bandwidth: Read</th> <th>Explanation</th> </tr> </thead> <tbody> <tr> <td><strong>Single token (Lth token, k beams)</strong></td> <td><code class="language-plaintext highlighter-rouge">k × (2 × d_model² + C)</code></td> <td><code class="language-plaintext highlighter-rouge">k × 2 × n_bytes × n_layers × n_heads × d_head × L</code></td> <td><code class="language-plaintext highlighter-rouge">k × 2 × n_bytes × n_layers × n_heads × d_head × (L-1)</code></td> <td>k separate sequences, each needs own KV cache and computation</td> </tr> <tr> <td><strong>Cumulative tokens (tokens 1, …, L, k beams)</strong></td> <td><code class="language-plaintext highlighter-rouge">k × (2 × d_model² × L + C × L)</code></td> <td><code class="language-plaintext highlighter-rouge">k × 2 × n_bytes × n_layers × n_heads × d_head × L</code></td> <td><code class="language-plaintext highlighter-rouge">k × 2 × n_bytes × n_layers × n_heads × d_head × L × (L-1)/2</code></td> <td>Memory can temporarily spike beyond k× during beam splitting when caches must be copied</td> </tr> </tbody> </table> <p><strong>Key Insight:</strong> Beam search with KV cache uses k× more memory than greedy decoding, but beam splitting during search can cause temporary memory explosions when multiple beams inherit from the same parent cache.</p> <h2 id="memory-bound-vs-flops-bound">Memory Bound vs. FLOPS Bound</h2> <h3 id="definitions">Definitions</h3> <p><strong>FLOPS Bound (Compute Bound):</strong> The bottleneck is arithmetic operations. GPU compute units are fully utilized, but memory can supply data faster than it’s consumed. Adding more compute power would speed up the process.</p> <p><strong>Memory Bound (Bandwidth Bound):</strong> The bottleneck is data movement. GPU compute units are idle waiting for data from memory. Memory bandwidth cannot supply data fast enough for the available compute. Adding more memory bandwidth would speed up the process.</p> <h3 id="how-to-identify">How to Identify</h3> <ul> <li><strong>FLOPS Bound</strong>: High GPU utilization (90%+), compute units busy</li> <li><strong>Memory Bound</strong>: Low GPU utilization despite high memory traffic, compute units waiting</li> </ul> <h3 id="kv-cache-impact">KV Cache Impact</h3> <p><strong>When FLOPS Bound (KV Cache Helps):</strong></p> <ul> <li>Without cache: Spend time computing Q,K,V for all tokens (6 × d_model² FLOPS)</li> <li>With cache: Only compute Q for new token (2 × d_model² FLOPS)</li> <li><strong>Result</strong>: 3× speedup because we eliminated computational work</li> </ul> <p><strong>When Memory Bound (KV Cache May Hurt):</strong></p> <ul> <li>Without cache: Read input tokens, compute everything fresh</li> <li>With cache: Read massive amounts of cached K,V data plus compute Q</li> <li><strong>Result</strong>: More memory traffic, potentially slower despite less computation</li> </ul> <p><strong>Example</strong>: Long sequences (1000+ tokens) often become memory bound because reading cached data dominates the time, making the FLOPS savings irrelevant.</p> <hr/> <h2 id="interview-questions">Interview Questions</h2> <h3 id="memory-architecture--scaling-1-8">Memory Architecture &amp; Scaling (1-8)</h3> <ol> <li> <p><strong>A sequence grows from 100→1000 tokens. How does cumulative memory bandwidth scale vs. final memory storage?</strong> <em>Final storage scales 10× linearly (1000 vs 100 tokens). Cumulative read bandwidth scales ~50× quadratically because total reads = sum of 1+2+…+999 vs 1+2+…+99. This explains why long sequences become memory bandwidth bound.</em></p> </li> <li> <p><strong>Why does the memory I/O formula have L×(L-1)/2 instead of just L²?</strong> <em>At token t, we read (t-1) previous tokens from cache. Total reads across all tokens = 0+1+2+…+(L-1) = L×(L-1)/2. This arithmetic series explains why reads grow quadratically while storage grows linearly.</em></p> </li> <li> <p><strong>Model: 32 layers, 32 heads, d_head=128, n_bytes=2. At what sequence length does KV cache exceed 1GB for batch_size=1?</strong> <em>Memory = 2×2×32×32×128×1×L = 524,288×L bytes. L = 1GB/524KB ≈ 2,000 tokens. This is why long conversations quickly exhaust GPU memory.</em></p> </li> <li> <p><strong>You have 24GB GPU memory. Compare max batch sizes: 500-token sequences vs 2000-token sequences (same model as above).</strong> <em>500 tokens: 24GB/(524KB×500) ≈ 92 sequences. 2000 tokens: 24GB/(524KB×2000) ≈ 23 sequences. Longer sequences drastically reduce concurrent user capacity.</em></p> </li> <li> <p><strong>A model spends 60% of time on memory reads during generation. What does this tell you about compute vs memory bandwidth?</strong> <em>Memory bandwidth is the bottleneck, not compute. GPU compute units are idle 60% of the time waiting for data. We know it’s bandwidth (not a slow computation) because reading cached data should be fast - if it takes 60% of time, the memory system can’t supply data fast enough for the compute units.</em></p> </li> <li> <p><strong>Beam search with 8 beams vs greedy with batch_size=8. One uses 10× more memory than expected. Which and why?</strong> <em>Beam search, if beam splitting causes cache copying. When beams diverge from same parent, each needs a full copy of the parent’s KV cache history, multiplying memory usage beyond the expected 8× baseline.</em></p> </li> <li> <p><strong>KV cache grows quadratically in practice but formula shows linear. What real-world factor causes this?</strong> <em>Beam search beam splitting, conversation branching with multiple completions, or poor memory management where cache entries aren’t properly shared/deduplicated. Implementation details matter enormously.</em></p> </li> <li> <p><strong>Memory-bandwidth calculation: Reading 1GB KV cache in 1ms requires what bandwidth?</strong> <em>1GB/1ms = 1TB/s. This is at the limit of current GPU memory bandwidth (~1-2TB/s), showing why long sequences become bandwidth bound rather than compute bound.</em></p> </li> </ol> <h3 id="system-design--trade-offs-9-16">System Design &amp; Trade-offs (9-16)</h3> <ol> <li> <p><strong>You’re serving a chatbot. User conversations average 50 tokens but 1% go to 10,000 tokens. How do you handle KV cache memory?</strong> <em>Implement sliding window attention (keep only last N tokens) or progressive offloading (move old cache entries to CPU/disk). The 1% tail drives 99% of memory costs, so optimize for the outliers.</em></p> </li> <li> <p><strong>A model uses Grouped Query Attention: 40 query heads, 8 KV heads. Memory savings vs standard attention?</strong> <em>Standard: n_heads=40 in formula. GQA: n_kv_heads=8. Memory reduction = (40-8)/40 = 80% savings. Query heads share the same 8 sets of keys/values, dramatically reducing cache size.</em></p> </li> <li> <p><strong>Production system: 95th percentile latency matters more than throughput. KV cache or large batches?</strong> <em>KV cache for consistent low latency per user, even if total throughput (users/sec) is lower. Large batches increase individual request latency due to queuing effects.</em></p> </li> <li> <p><strong>Two GPUs: 16GB with 2TB/s bandwidth vs 32GB with 1TB/s bandwidth. Which is better for long conversations?</strong> <em>Depends on sequence length. Short sequences (memory bandwidth bound): 16GB GPU wins. Long sequences (memory capacity bound): 32GB GPU wins. The crossover point depends on your specific workload.</em></p> </li> <li> <p><strong>Inference cost optimization: KV cache uses 2× memory but 3× faster generation. At what utilization rate do you break even?</strong> <em>If you can keep GPUs 66%+ utilized with KV cache (⅔ of capacity), the 3× speed improvement compensates for 2× memory cost. Below 66% utilization, large batches without KV cache may be more cost-effective.</em></p> </li> <li> <p><strong>User requests 5 different continuations of same prompt. KV cache strategy?</strong> <em>Cache the common prompt prefix once, then branch KV cache only for the different continuations. This avoids recomputing the shared 90% of work while only duplicating the divergent portions.</em></p> </li> <li> <p><strong>Why might a model perform WORSE with KV cache enabled in some scenarios?</strong> <em>Memory pressure causing GPU memory swapping to CPU, memory bandwidth saturation slowing all operations, or cache management overhead (copying, allocation) exceeding computational savings for short sequences.</em></p> </li> <li> <p><strong>Distributed inference: How do you handle KV cache across multiple GPUs?</strong> <em>Model parallelism: partition cache by layers across GPUs, but attention requires all-gather communication. Sequence parallelism is challenging due to autoregressive dependencies. Pipeline parallelism works but complicates cache management.</em></p> </li> </ol> <h3 id="advanced-concepts--edge-cases-17-25">Advanced Concepts &amp; Edge Cases (17-25)</h3> <ol> <li> <p><strong>Flash Attention reduces memory usage. How does this interact with KV cache benefits?</strong> <em>Flash Attention optimizes attention computation memory (intermediate activations), while KV cache optimizes recomputation across time steps. They’re complementary - Flash Attention reduces per-step memory, KV cache reduces cross-step computation.</em></p> </li> <li> <p><strong>In transformer training vs inference: why is KV cache irrelevant for training?</strong> <em>Training uses teacher forcing - processes entire sequences in parallel with full attention matrices. No autoregressive generation step-by-step, so no opportunity to reuse previous computations.</em></p> </li> <li> <p><strong>Model uses rotary positional embeddings (RoPE). How does this affect what we cache?</strong> <em>Cache unrotated K,V vectors and apply position-dependent rotation during attention computation. This is because the rotation depends on the relative position between query and key, which changes for each new token.</em></p> </li> <li> <p><strong>A sequence has repeated phrases. Could we compress KV cache by deduplicating similar K,V vectors?</strong> <em>Theoretically possible, but positional embeddings make even identical tokens have different representations. The attention mechanism depends on position, not just content, making deduplication complex and potentially harmful to model quality.</em></p> </li> <li> <p><strong>Mixture of Experts (MoE) model: different tokens activate different experts. How does this complicate KV cache?</strong> <em>Each expert may need separate KV caches if they have different dimensions, or the routing decisions affect which cached values are relevant. Cache size scales with number of active experts per token.</em></p> </li> <li> <p><strong>Streaming generation: user types while model generates. How do you update KV cache mid-generation?</strong> <em>Invalidate cache from the interruption point onward, recompute from the user’s new input position. This requires careful bookkeeping of which cache entries correspond to which part of the conversation state.</em></p> </li> <li> <p><strong>Speculative decoding: generate multiple tokens in parallel, then verify. How does KV cache work here?</strong> <em>Cache optimistically for all speculated tokens, but maintain checkpoints to rollback if speculation fails. This creates a tree of potential cache states that must be managed efficiently.</em></p> </li> <li> <p><strong>Model pruning removes 50% of attention heads. How does this affect KV cache memory and performance trade-offs?</strong> <em>Memory usage halves (n_heads reduces by 50%), but attention quality may degrade, potentially requiring longer sequences for same performance. Need to rebalance the memory savings vs quality trade-off.</em></p> </li> <li> <p><strong>We store K,V per head in KV cache. Do we also need to store the concatenated output after multi-head attention?</strong> <em>No, only store individual K,V per head. The concatenated output gets recomputed each time because it depends on the new query. KV cache only stores the reusable components (keys and values), not query-dependent results.</em></p> </li> <li> <p><strong>When we say “sequence length grows linearly,” do we mean input length or total length including generated tokens?</strong> <em>Total length including all generated tokens. During autoregressive generation, sequence length = original_prompt + tokens_generated_so_far. Each new token increases this total length by 1.</em></p> </li> <li> <p><strong>Batch processing sequences of different lengths with padding: do shorter sequences waste KV cache memory?</strong> <em>In naive implementations, yes - all sequences get padded to max_length, wasting memory. Better implementations use attention masks to ignore padding during computation and variable-length caching to avoid storing padding positions in the KV cache.</em></p> </li> <li> <p><strong>Grouped Query Attention (GQA): multiple heads share K,V. How does this change the storage formula?</strong> <em>Formula becomes: 2 × n_bytes × n_layers × n_kv_heads × d_head (where n_kv_heads &lt; n_heads). If 32 query heads share 8 KV heads, you get 4× memory savings while maintaining most of the attention quality.</em></p> </li> <li> <p><strong>Beam search: each beam needs its own KV cache. How does this explode memory?</strong> <em>Memory scales as k_beams × sequence_length × cache_size. With 8 beams and 1000 tokens, you need 8× more memory than greedy decoding. When beams split from same parent, cache copying creates temporary memory spikes beyond this 8× baseline.</em></p> </li> <li> <p><strong>Calculate KV cache FLOPS savings: batch_size=B, context_length=L, generate next_n tokens. What’s the speedup?</strong> <em>Without cache: 6×B×(L+next_n)×d_model² total FLOPS. With cache: 6×B×L×d_model² (initial) + 2×B×next_n×d_model² (generation). Speedup = 3×(L+next_n)/(3×L+next_n). For L»next_n, approaches 3× speedup.</em></p> </li> <li> <p><strong>What memory and time overheads does KV cache add?</strong> <em>Memory: 2×n_bytes×n_layers×n_heads×d_head×B×L storage overhead. Time: memory allocation/deallocation, cache management, and quadratic read bandwidth growth (reading more cached data for each new token). These can outweigh benefits for short sequences.</em></p> </li> <li> <p><strong>Can we be memory-bound during forward pass but FLOPS-bound during sampling?</strong> <em>Yes! Forward pass reads massive amounts of cached K,V data (memory-bound). Sampling step does text generation, top-k filtering, probabilistic sampling (FLOPS-bound). Different parts of inference have different bottlenecks depending on sequence length and model size.</em></p> </li> </ol>]]></content><author><name></name></author><category term="explain-paper"/><category term="dynamic-programming"/><category term="cs-theory"/><category term="algorithms"/><category term="complexity"/><category term="comments"/><summary type="html"><![CDATA[We prove you can solve dynamic programming problems polynomially faster if you have a simple cost function for kD LWS problems.]]></summary></entry></feed>