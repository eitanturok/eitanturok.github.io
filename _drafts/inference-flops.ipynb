{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a59cb79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from functools import partial\n",
    "\n",
    "from tinygrad import Tensor, nn\n",
    "from tinygrad.nn.state import get_parameters\n",
    "from tinygrad.helpers import Timing, GlobalCounters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc7dd76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_kv(x:Tensor, n_rep:int) -> Tensor:\n",
    "  bs, seqlen, n_kv_heads, head_dim = x.shape\n",
    "  if n_rep == 1: return x\n",
    "  # NOTE: this is different from x.repeat((1, 1, n_rep, 1))\n",
    "  return x.repeat((1, 1, 1, n_rep)).reshape(bs, seqlen, n_kv_heads * n_rep, head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee66d53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention:\n",
    "  def __init__(self, dim, n_heads, n_kv_heads=None, max_context=1024, linear=nn.Linear):\n",
    "    self.n_heads = n_heads\n",
    "    self.n_kv_heads = n_kv_heads if n_kv_heads is not None else n_heads # n_kv_heads != n_heads implies MQA [arxiv/2307.09288, A.2.1]\n",
    "    self.head_dim = dim // n_heads\n",
    "    self.n_rep = self.n_heads // self.n_kv_heads\n",
    "    self.max_context = max_context\n",
    "\n",
    "    self.wq = linear(dim, self.n_heads * self.head_dim, bias=False)\n",
    "    self.wk = linear(dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "    self.wv = linear(dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "    self.wo = linear(self.n_heads * self.head_dim, dim, bias=False)\n",
    "\n",
    "  def __call__(self, x:Tensor, start_pos:int, mask:Optional[Tensor]=None) -> Tensor:\n",
    "      xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n",
    "      xq = xq.reshape(xq.shape[0], xq.shape[1], self.n_heads, self.head_dim)\n",
    "      xk = xk.reshape(xk.shape[0], xk.shape[1], self.n_kv_heads, self.head_dim)\n",
    "      xv = xv.reshape(xv.shape[0], xv.shape[1], self.n_kv_heads, self.head_dim)\n",
    "\n",
    "      # todo: add RoPE\n",
    "      bsz, seqlen, _, _ = xq.shape\n",
    "\n",
    "      # create kv cache\n",
    "      if not hasattr(self, \"cache_kv\"):\n",
    "        self.cache_kv = Tensor.zeros(2, bsz, self.max_context, self.n_kv_heads, self.head_dim, dtype=x.dtype).contiguous().realize()\n",
    "\n",
    "      # update the cache\n",
    "      self.cache_kv[:, :, start_pos:start_pos+seqlen, :, :].assign(Tensor.stack(xk, xv)).realize()\n",
    "\n",
    "      keys = self.cache_kv[0, :, 0:start_pos+seqlen, :, :]\n",
    "      values = self.cache_kv[1, :, 0:start_pos+seqlen, :, :]\n",
    "\n",
    "      keys, values = repeat_kv(keys, self.n_rep), repeat_kv(values, self.n_rep)\n",
    "      xq, keys, values = xq.transpose(1, 2), keys.transpose(1, 2), values.transpose(1, 2)\n",
    "      attn = xq.scaled_dot_product_attention(keys, values, mask).transpose(1, 2)\n",
    "      attn = attn.reshape(bsz, seqlen, -1)\n",
    "      return self.wo(attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55186666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(st:float, et:float):\n",
    "    dur = GlobalCounters.time_sum_s-st\n",
    "    msg = f\", {dur*1e3:.2f} ms on GPU\"\n",
    "    msg += f\", {GlobalCounters.global_ops*1e-9:.2f} GOPs\"\n",
    "    msg += f\", {GlobalCounters.global_mem*1e-9:.2f} GB\"\n",
    "    msg += f\", {(0.0 if GlobalCounters.global_mem == 0.0 else GlobalCounters.global_ops / GlobalCounters.global_mem):.2f} OPs/byte\"\n",
    "    # msg += f\", {(0.0 if dur == 0 else GlobalCounters.global_mem*1e-9/dur):.2f} GB/s\"\n",
    "    # msg += f\", param {(0.0 if dur == 0 else param_bytes*1e-9/dur):.2f} GB/s\"\n",
    "    return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9835b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model params take up 0.016777216 GB\n",
      "(1024, 1024)\n",
      "1048576\n",
      "4\n",
      "(1024, 1024)\n",
      "1048576\n",
      "4\n",
      "(1024, 1024)\n",
      "1048576\n",
      "4\n",
      "(1024, 1024)\n",
      "1048576\n",
      "4\n",
      "Realize Model (no kv cache) in503.80 ms, 0.00 ms on GPU, 0.93 GOPs, 0.13 GB, 6.96 OPs/byte\n",
      "Attention in 178.00 ms, 0.00 ms on GPU, 1.00 GOPs, 0.17 GB, 5.95 OPs/byte\n"
     ]
    }
   ],
   "source": [
    "dim = 1024\n",
    "n_heads = 64\n",
    "max_context = 1024\n",
    "model = Attention(dim, n_heads, max_context=max_context)\n",
    "param_bytes = sum(x.lazydata.size * x.dtype.itemsize for x in get_parameters(model))\n",
    "print(f\"model params take up {param_bytes*1e-9} GB\")\n",
    "\n",
    "GlobalCounters.reset()\n",
    "st = GlobalCounters.time_sum_s\n",
    "with Timing(\"Realize Model (no kv cache) in\", on_exit=partial(stats, st)):\n",
    "    # realize wq, wk, wv, wo\n",
    "    for x in get_parameters(model):\n",
    "        x.realize()\n",
    "        print(x.shape)\n",
    "        print(x.lazydata.size)\n",
    "        print(x.lazydata.dtype.itemsize)\n",
    "\n",
    "bsz, seqlen, dim = 1, 8, 1024\n",
    "start_pos = 0\n",
    "\n",
    "# with Timing(\"total \", on_exit=lambda x: f\", {1e9/x:.2f} tok/s, {GlobalCounters.global_mem/x:.2f} GB/s, param {param_bytes/x:.2f} GB/s\"):\n",
    "with Timing(\"Attention in \", on_exit=partial(stats, st)):\n",
    "    x = Tensor.empty(bsz, seqlen, dim)\n",
    "    out = model(x, start_pos)\n",
    "    out.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "602dc142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 1024)\n",
      "(1024, 1024)\n",
      "(1024, 1024)\n",
      "(1024, 1024)\n",
      "(2, 1, 1024, 64, 16)\n"
     ]
    }
   ],
   "source": [
    "for x in get_parameters(model):\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1967ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param bytes: 0.01 GB\n",
      "global mem: 0.07 GB\n",
      "mem used: 0.01 GB\n"
     ]
    }
   ],
   "source": [
    "class MLP:\n",
    "    def __init__(self, d1, d2, d3):\n",
    "        self.l1, self.l2 = nn.Linear(d1, d2),  nn.Linear(d2, d3)\n",
    "    def __call__(self, x): return self.l2(self.l1(x).relu())\n",
    "\n",
    "model = MLP(1024, 2048, 10)\n",
    "param_bytes = sum([x.lazydata.size * x.dtype.itemsize for x in get_parameters(model)])\n",
    "print(f'param bytes: {param_bytes*1e-9:.2f} GB')\n",
    "\n",
    "GlobalCounters.reset()\n",
    "for x in get_parameters(model): x.realize()\n",
    "print(f\"global mem: {GlobalCounters.global_mem*1e-9:.2f} GB\")\n",
    "print(f\"mem used: {GlobalCounters.mem_used*1e-9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d079b7d9",
   "metadata": {},
   "source": [
    "I noticed there is a difference between the bytes allocated for the model parameters and the bytes allocated globally for everything. Specifically, why do we allocate 0.06 GBs more for the global memory than for just the model params? What is taking up this extra memory? Where does this overhead come from? I tried reseting the global memory but that didn't work. Is this because we have also allocated space for all of the nested uops and all the complexity of the tinygrad stack?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c16f085",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinygrad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
